{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: pointcloud_0000.pcd\n",
      "Target: pointcloud_0004.pcd\n",
      "Source point cloud has 22915 points\n",
      "Target point cloud has 22904 points\n",
      "Visualizing original point clouds (red: source, green: target)\n",
      "Estimating normals for ICP...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prepare_point_clouds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m visualize_point_clouds(source, target)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Generate initial transformation and run ICP\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m transformation, initial_fitness, initial_rmse, final_fitness, final_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mrun_icp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Transform the source point cloud using the estimated transformation\u001b[39;00m\n\u001b[0;32m    147\u001b[0m source_transformed \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(source)\n",
      "Cell \u001b[1;32mIn[2], line 78\u001b[0m, in \u001b[0;36mrun_icp\u001b[1;34m(source, target, initial_transform, max_iteration, threshold, estimate_normals, knn, voxel_size)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mhas_normals() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target\u001b[38;5;241m.\u001b[39mhas_normals():\n\u001b[0;32m     77\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimating normals for ICP...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m      source_processed, target_processed \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_point_clouds\u001b[49m(\n\u001b[0;32m     79\u001b[0m          source, target, voxel_size\u001b[38;5;241m=\u001b[39mvoxel_size, estimate_normals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, knn\u001b[38;5;241m=\u001b[39mknn)\n\u001b[0;32m     80\u001b[0m      \u001b[38;5;66;03m# Use processed clouds ONLY if normals weren't already present\u001b[39;00m\n\u001b[0;32m     81\u001b[0m      \u001b[38;5;66;03m# This assumes normals might be estimated earlier (e.g., for RANSAC)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m      \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mhas_normals(): source \u001b[38;5;241m=\u001b[39m source_processed\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prepare_point_clouds' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import copy\n",
    "import os\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "# Function to load point clouds\n",
    "def load_point_cloud(file_path):\n",
    "    pcd = o3d.io.read_point_cloud(file_path)\n",
    "    return pcd\n",
    "\n",
    "# Function to visualize point clouds\n",
    "def visualize_point_clouds(source, target, source_transformed=None):\n",
    "    vis_list = []\n",
    "    \n",
    "    # Original source in red\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    source_temp.paint_uniform_color([1, 0, 0])  # Red for source\n",
    "    vis_list.append(source_temp)\n",
    "    \n",
    "    # Target in green\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    target_temp.paint_uniform_color([0, 1, 0])  # Green for target\n",
    "    vis_list.append(target_temp)\n",
    "    \n",
    "    # Transformed source in blue (if provided)\n",
    "    if source_transformed is not None:\n",
    "        source_transformed_temp = copy.deepcopy(source_transformed)\n",
    "        source_transformed_temp.paint_uniform_color([0, 0, 1])  # Blue for transformed source\n",
    "        vis_list.append(source_transformed_temp)\n",
    "    \n",
    "    o3d.visualization.draw_geometries(vis_list)\n",
    "\n",
    "# # Function to run ICP\n",
    "# def point_to_point_icp(source, target, initial_transform=None, max_iteration=30, threshold=0.02):\n",
    "#     if initial_transform is None:\n",
    "#         # Create a valid initial transformation matrix with rotation and translation\n",
    "#         # Using scipy's ortho_group to generate a random orthonormal matrix for rotation\n",
    "#         R = ortho_group.rvs(3)\n",
    "#         t = np.random.rand(3, 1) * 0.5  # Small random translation\n",
    "        \n",
    "#         initial_transform = np.eye(4)\n",
    "#         initial_transform[:3, :3] = R\n",
    "#         initial_transform[:3, 3:] = t\n",
    "    \n",
    "#     # Evaluate initial alignment\n",
    "#     evaluation = o3d.pipelines.registration.evaluate_registration(\n",
    "#         source, target, threshold, initial_transform)\n",
    "#     initial_fitness = evaluation.fitness\n",
    "#     initial_rmse = evaluation.inlier_rmse\n",
    "    \n",
    "#     print(f\"Initial alignment - Fitness: {initial_fitness}, RMSE: {initial_rmse}\")\n",
    "    \n",
    "#     # Run ICP\n",
    "#     reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "#         source, target, threshold, initial_transform,\n",
    "#         o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "#         o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=max_iteration))\n",
    "    \n",
    "#     # Get results\n",
    "#     transformation = reg_p2p.transformation\n",
    "#     fitness = reg_p2p.fitness\n",
    "#     inlier_rmse = reg_p2p.inlier_rmse\n",
    "    \n",
    "#     print(f\"ICP registration - Fitness: {fitness}, RMSE: {inlier_rmse}\")\n",
    "#     print(f\"Transformation matrix:\\n{transformation}\")\n",
    "    \n",
    "#     return transformation, initial_fitness, initial_rmse, fitness, inlier_rmse\n",
    "\n",
    "def run_icp(source, target, initial_transform=None, \n",
    "            max_iteration=100, threshold=0.05, #<-- Adjusted defaults maybe\n",
    "            estimate_normals=True, knn=30, voxel_size=0.05): # <-- Pass voxel_size\n",
    "    \"\"\"Run Point-to-Plane ICP\"\"\"\n",
    "    \n",
    "    # Ensure point clouds are processed with normals\n",
    "    if not source.has_normals() or not target.has_normals():\n",
    "         print(\"Estimating normals for ICP...\")\n",
    "         source_processed, target_processed = prepare_point_clouds(\n",
    "             source, target, voxel_size=voxel_size, estimate_normals=True, knn=knn)\n",
    "         # Use processed clouds ONLY if normals weren't already present\n",
    "         # This assumes normals might be estimated earlier (e.g., for RANSAC)\n",
    "         if not source.has_normals(): source = source_processed\n",
    "         if not target.has_normals(): target = target_processed\n",
    "    else:\n",
    "         # If normals exist, maybe still downsample for performance? Optional.\n",
    "         # source = source.voxel_down_sample(voxel_size)\n",
    "         # target = target.voxel_down_sample(voxel_size)\n",
    "         pass # Assume they are ready if they have normals\n",
    "\n",
    "    if initial_transform is None:\n",
    "        print(\"Warning: initial_transform is None in run_icp. Using random guess.\")\n",
    "        initial_transform = get_random_orthogonal_matrix() # Fallback, should be avoided\n",
    "\n",
    "    # Evaluate initial alignment (optional, for logging)\n",
    "    evaluation = o3d.pipelines.registration.evaluate_registration(\n",
    "        source, target, threshold, initial_transform)\n",
    "    initial_fitness = evaluation.fitness\n",
    "    initial_rmse = evaluation.inlier_rmse\n",
    "    # print(f\"ICP Initial alignment - Fitness: {initial_fitness}, RMSE: {initial_rmse}\") # Optional log\n",
    "\n",
    "    # --- Use Point-to-Plane ---\n",
    "    estimation_method = o3d.pipelines.registration.TransformationEstimationPointToPlane()\n",
    "    # --------------------------\n",
    "\n",
    "    # Run ICP\n",
    "    reg_result = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, threshold, initial_transform,\n",
    "        estimation_method, # <-- Use Point-to-Plane\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=max_iteration))\n",
    "    \n",
    "    # Get results\n",
    "    transformation = reg_result.transformation\n",
    "    fitness = reg_result.fitness\n",
    "    inlier_rmse = reg_result.inlier_rmse\n",
    "    \n",
    "    return transformation, initial_fitness, initial_rmse, fitness, inlier_rmse\n",
    "\n",
    "# Now, let's use these functions to load two consecutive point clouds and perform ICP registration:\n",
    "dataset_dir = f\"C:/Users/aarya/Github_Projects/Striver_A2Z/cv_bonus/selected_pcds\"  # Change this to your dataset path\n",
    " \n",
    "# Get sorted list of point cloud files\n",
    "pcd_files = sorted([f for f in os.listdir(dataset_dir) if f.startswith(\"pointcloud_\") and f.endswith(\".pcd\")])\n",
    "\n",
    "# Select two consecutive point clouds (e.g., the first two)\n",
    "source_file = os.path.join(dataset_dir, pcd_files[0])\n",
    "target_file = os.path.join(dataset_dir, pcd_files[1])\n",
    "\n",
    "print(f\"Source: {pcd_files[0]}\")\n",
    "print(f\"Target: {pcd_files[1]}\")\n",
    "\n",
    "# Load point clouds\n",
    "source = load_point_cloud(source_file)\n",
    "target = load_point_cloud(target_file)\n",
    "\n",
    "# Print basic information about the point clouds\n",
    "print(f\"Source point cloud has {len(source.points)} points\")\n",
    "print(f\"Target point cloud has {len(target.points)} points\")\n",
    "\n",
    "# Visualize original point clouds\n",
    "print(\"Visualizing original point clouds (red: source, green: target)\")\n",
    "visualize_point_clouds(source, target)\n",
    "\n",
    "# Generate initial transformation and run ICP\n",
    "transformation, initial_fitness, initial_rmse, final_fitness, final_rmse = run_icp(source, target)\n",
    "\n",
    "# Transform the source point cloud using the estimated transformation\n",
    "source_transformed = copy.deepcopy(source)\n",
    "source_transformed.transform(transformation)\n",
    "\n",
    "# Visualize the result\n",
    "print(\"Visualizing point clouds after ICP (red: original source, green: target, blue: transformed source)\")\n",
    "visualize_point_clouds(source, target, source_transformed)\n",
    "\n",
    "# Report results\n",
    "print(\"\\nPoint-to-Point ICP Results:\")\n",
    "print(f\"Initial alignment - Fitness: {initial_fitness:.6f}, RMSE: {initial_rmse:.6f}\")\n",
    "print(f\"Final alignment - Fitness: {final_fitness:.6f}, RMSE: {final_rmse:.6f}\")\n",
    "print(f\"Improvement - Fitness: {final_fitness - initial_fitness:.6f}, RMSE: {initial_rmse - final_rmse:.6f}\")\n",
    "\n",
    "# Now, let's extend the code to register multiple point clouds and reconstruct the TurtleBot's trajectory:\n",
    "def register_point_cloud_sequence(pcd_files, dataset_dir, max_iteration=30, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Register a sequence of point clouds and compute the trajectory\n",
    "    \"\"\"\n",
    "    # List to store trajectory (poses)\n",
    "    trajectory = []\n",
    "    \n",
    "    # List to store registered point clouds\n",
    "    registered_pcds = []\n",
    "    \n",
    "    # Load the first point cloud\n",
    "    first_pcd = load_point_cloud(os.path.join(dataset_dir, pcd_files[0]))\n",
    "    \n",
    "    # Add the first point cloud to the list of registered point clouds\n",
    "    registered_pcds.append(copy.deepcopy(first_pcd))\n",
    "    \n",
    "    # Add the identity matrix as the first pose (global frame is set to the first point cloud)\n",
    "    trajectory.append(np.eye(4))\n",
    "    \n",
    "    # Global transformation (accumulated)\n",
    "    global_transform = np.eye(4)\n",
    "    \n",
    "    # Process each consecutive pair of point clouds\n",
    "    for i in range(1, len(pcd_files)):\n",
    "        print(f\"\\nProcessing point clouds {i-1} and {i}...\")\n",
    "        \n",
    "        # Load the current point cloud\n",
    "        current_pcd = load_point_cloud(os.path.join(dataset_dir, pcd_files[i]))\n",
    "        \n",
    "        # Previous registered point cloud\n",
    "        prev_pcd = registered_pcds[-1]\n",
    "        \n",
    "        # Initial transformation (small perturbation around identity)\n",
    "        R = ortho_group.rvs(3)  # Random rotation\n",
    "        t = np.random.rand(3, 1) * 0.1  # Small random translation\n",
    "        \n",
    "        initial_transform = np.eye(4)\n",
    "        initial_transform[:3, :3] = R\n",
    "        initial_transform[:3, 3:] = t\n",
    "        \n",
    "        # Run ICP to find the transformation from current to previous\n",
    "        transformation, _, _, fitness, rmse = run_icp(\n",
    "            current_pcd, prev_pcd, initial_transform, max_iteration, threshold)\n",
    "        \n",
    "        # Update the global transformation\n",
    "        global_transform = global_transform @ transformation\n",
    "        \n",
    "        # Add the global transformation to the trajectory\n",
    "        trajectory.append(copy.deepcopy(global_transform))\n",
    "        \n",
    "        # Transform the current point cloud to the global frame\n",
    "        transformed_pcd = copy.deepcopy(current_pcd)\n",
    "        transformed_pcd.transform(global_transform)\n",
    "        \n",
    "        # Add the transformed point cloud to the list of registered point clouds\n",
    "        registered_pcds.append(transformed_pcd)\n",
    "        \n",
    "        print(f\"Registration {i}/{len(pcd_files)-1} - Fitness: {fitness:.6f}, RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    return registered_pcds, trajectory\n",
    "\n",
    "# Function to visualize the trajectory\n",
    "def visualize_trajectory(trajectory):\n",
    "    \"\"\"\n",
    "    Visualize the trajectory as a line set\n",
    "    \"\"\"\n",
    "    # Create a LineSet to represent the trajectory\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    \n",
    "    # Extract the translation part of each transformation matrix\n",
    "    points = np.array([T[:3, 3] for T in trajectory])\n",
    "    \n",
    "    # Set the points\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    \n",
    "    # Create lines connecting consecutive points\n",
    "    lines = [[i, i + 1] for i in range(len(points) - 1)]\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    \n",
    "    # Set colors (red)\n",
    "    colors = [[1, 0, 0] for _ in range(len(lines))]\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    return line_set\n",
    "\n",
    "# Function to visualize the registered point cloud map and trajectory\n",
    "def visualize_map_and_trajectory(registered_pcds, trajectory):\n",
    "    \"\"\"\n",
    "    Visualize the registered point cloud map and the trajectory\n",
    "    \"\"\"\n",
    "    # Create a combined point cloud map with random colors for each original cloud\n",
    "    map_vis = []\n",
    "    \n",
    "    for i, pcd in enumerate(registered_pcds):\n",
    "        pcd_colored = copy.deepcopy(pcd)\n",
    "        # Assign a random color to each point cloud\n",
    "        color = np.random.rand(3)\n",
    "        pcd_colored.paint_uniform_color(color)\n",
    "        map_vis.append(pcd_colored)\n",
    "    \n",
    "    # Create trajectory visualization\n",
    "    trajectory_line = visualize_trajectory(trajectory)\n",
    "    map_vis.append(trajectory_line)\n",
    "    \n",
    "    # Visualize\n",
    "    o3d.visualization.draw_geometries(map_vis)\n",
    "\n",
    "def main():\n",
    "    # Set the dataset directory path\n",
    "    dataset_dir = \"C:/Users/aarya/Github_Projects/Striver_A2Z/cv_bonus/selected_pcds\"  # Change this to your dataset path\n",
    "    \n",
    "    # Get sorted list of point cloud files\n",
    "    pcd_files = sorted([f for f in os.listdir(dataset_dir) if f.startswith(\"pointcloud_\") and f.endswith(\".pcd\")])\n",
    "    \n",
    "    print(f\"Found {len(pcd_files)} point cloud files.\")\n",
    "    \n",
    "    # Parameters for ICP\n",
    "    max_iteration = 400\n",
    "    threshold = 0.02\n",
    "    \n",
    "    # Register the point cloud sequence\n",
    "    registered_pcds, trajectory = register_point_cloud_sequence(\n",
    "        pcd_files, dataset_dir, max_iteration, threshold)\n",
    "    \n",
    "    print(\"\\nRegistration complete!\")\n",
    "    print(f\"Registered {len(registered_pcds)} point clouds.\")\n",
    "    print(f\"Trajectory has {len(trajectory)} poses.\")\n",
    "    \n",
    "    # Visualize the map and trajectory\n",
    "    print(\"\\nVisualizing the registered point cloud map and trajectory...\")\n",
    "    visualize_map_and_trajectory(registered_pcds, trajectory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import copy\n",
    "import os\n",
    "from scipy.stats import ortho_group\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "def load_point_cloud(file_path):\n",
    "    \"\"\"Load a point cloud from file and preprocess if needed\"\"\"\n",
    "    pcd = o3d.io.read_point_cloud(file_path)\n",
    "    return pcd\n",
    "\n",
    "def preprocess_point_cloud(pcd, voxel_size=0.05, estimate_normals=False, knn=30):\n",
    "    \"\"\"Preprocess point cloud with optional downsampling and normal estimation\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    processed_pcd = copy.deepcopy(pcd)\n",
    "    \n",
    "    # Downsample using voxel grid filter\n",
    "    processed_pcd = processed_pcd.voxel_down_sample(voxel_size)\n",
    "    \n",
    "    # Estimate normals if requested\n",
    "    if estimate_normals:\n",
    "        search_param = o3d.geometry.KDTreeSearchParamKNN(knn=knn)\n",
    "        processed_pcd.estimate_normals(search_param)\n",
    "        # Orient normals consistently\n",
    "        processed_pcd.orient_normals_consistent_tangent_plane(k=knn)\n",
    "    \n",
    "    return processed_pcd\n",
    "\n",
    "def prepare_point_clouds(source, target, voxel_size=0.05, estimate_normals=False, knn=30):\n",
    "    \"\"\"Prepare point clouds for registration\"\"\"\n",
    "    source_processed = preprocess_point_cloud(source, voxel_size, estimate_normals, knn)\n",
    "    target_processed = preprocess_point_cloud(target, voxel_size, estimate_normals, knn)\n",
    "    \n",
    "    return source_processed, target_processed\n",
    "\n",
    "def get_random_orthogonal_matrix():\n",
    "    \"\"\"Generate a random orthogonal rotation matrix and add small random translation\"\"\"\n",
    "    R = ortho_group.rvs(3)\n",
    "    t = np.random.rand(3, 1) * 0.5  # Small random translation\n",
    "    \n",
    "    initial_transform = np.eye(4)\n",
    "    initial_transform[:3, :3] = R\n",
    "    initial_transform[:3, 3:] = t\n",
    "    \n",
    "    return initial_transform\n",
    "\n",
    "def get_initial_transform_from_ransac(source, target, voxel_size, with_normals=False):\n",
    "    \"\"\"Use RANSAC for initial alignment\"\"\"\n",
    "    # Prepare features for feature matching\n",
    "    if with_normals:\n",
    "        source_fpfh = o3d.pipelines.registration.compute_fpfh_feature(source, \n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "        target_fpfh = o3d.pipelines.registration.compute_fpfh_feature(target, \n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "        \n",
    "        # Use RANSAC for global registration\n",
    "        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "            source, target, source_fpfh, target_fpfh, \n",
    "            mutual_filter=True,\n",
    "            max_correspondence_distance=voxel_size * 1.5,\n",
    "            estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "            ransac_n=3,\n",
    "            checkers=[\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(voxel_size * 1.5)\n",
    "            ],\n",
    "            criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n",
    "        )\n",
    "        return result.transformation\n",
    "    else:\n",
    "        # Without normals, use a simpler approach\n",
    "        result = o3d.pipelines.registration.registration_ransac_based_on_correspondence(\n",
    "            source, target,\n",
    "            o3d.pipelines.registration.pick_correspondences(source, target, 0.1, 0.1),\n",
    "            max_correspondence_distance=voxel_size * 1.5,\n",
    "            estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "            ransac_n=3,\n",
    "            criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 1000)\n",
    "        )\n",
    "        return result.transformation\n",
    "\n",
    "def point_to_point_icp(source, target, initial_transform=None, \n",
    "                        max_iteration=30, threshold=0.02, \n",
    "                        estimate_normals=False, knn=30):\n",
    "    \"\"\"Run ICP with various configurations\"\"\"\n",
    "    # Process point clouds if needed\n",
    "    if estimate_normals:\n",
    "        source_processed, target_processed = prepare_point_clouds(\n",
    "            source, target, voxel_size=0.05, estimate_normals=True, knn=knn)\n",
    "    else:\n",
    "        source_processed, target_processed = source, target\n",
    "    \n",
    "    # Default initial transform if not provided\n",
    "    if initial_transform is None:\n",
    "        initial_transform = get_random_orthogonal_matrix()\n",
    "    \n",
    "    # Evaluate initial alignment\n",
    "    evaluation = o3d.pipelines.registration.evaluate_registration(\n",
    "        source_processed, target_processed, threshold, initial_transform)\n",
    "    initial_fitness = evaluation.fitness\n",
    "    initial_rmse = evaluation.inlier_rmse\n",
    "    \n",
    "    # Run ICP\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_processed, target_processed, threshold, initial_transform,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=max_iteration))\n",
    "    \n",
    "    # Get results\n",
    "    transformation = reg_p2p.transformation\n",
    "    fitness = reg_p2p.fitness\n",
    "    inlier_rmse = reg_p2p.inlier_rmse\n",
    "    \n",
    "    # Calculate transformation error (Frobenius norm of difference)\n",
    "    # Note: In practice, this would be compared against ground truth if available\n",
    "    transform_error = np.linalg.norm(transformation - initial_transform)\n",
    "    \n",
    "    return {\n",
    "        'transformation': transformation,\n",
    "        'initial_fitness': initial_fitness,\n",
    "        'initial_rmse': initial_rmse,\n",
    "        'final_fitness': fitness,\n",
    "        'final_rmse': inlier_rmse,\n",
    "        'fitness_improvement': fitness - initial_fitness,\n",
    "        'rmse_improvement': initial_rmse - inlier_rmse,\n",
    "        'transform_error': transform_error\n",
    "    }\n",
    "\n",
    "def run_experiments(source_file, target_file):\n",
    "    \"\"\"Run multiple experiments with different hyperparameters\"\"\"\n",
    "    # Load point clouds\n",
    "    source = load_point_cloud(source_file)\n",
    "    target = load_point_cloud(target_file)\n",
    "    \n",
    "    # Define hyperparameter combinations to test\n",
    "    thresholds = [0.01, 0.1, 0.2, 0.5, 1]\n",
    "    # max_iterations = [30, 50, 100]\n",
    "    # max_iterations = [4000]\n",
    "    max_iterations = [400]\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    \n",
    "    # 1. Experiments with different thresholds and random orthogonal matrix initialization\n",
    "    print(\"Running experiments with random orthogonal matrix initialization...\")\n",
    "    for threshold in thresholds:\n",
    "        for max_iter in max_iterations:\n",
    "            # Run without normals\n",
    "            print(f\"Threshold: {threshold}, Max Iterations: {max_iter}, Without Normals\")\n",
    "            initial_transform = get_random_orthogonal_matrix()\n",
    "            result = point_to_point_icp(source, target, initial_transform, \n",
    "                                       max_iter, threshold, estimate_normals=False)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'initialization': 'Random Orthogonal',\n",
    "                'threshold': threshold,\n",
    "                'max_iterations': max_iter,\n",
    "                'with_normals': False,\n",
    "                **result\n",
    "            })\n",
    "            \n",
    "            # Run with normals (knn=30)\n",
    "            print(f\"Threshold: {threshold}, Max Iterations: {max_iter}, With Normals (knn=30)\")\n",
    "            result = point_to_point_icp(source, target, initial_transform, \n",
    "                                       max_iter, threshold, estimate_normals=True, knn=30)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'initialization': 'Random Orthogonal',\n",
    "                'threshold': threshold,\n",
    "                'max_iterations': max_iter,\n",
    "                'with_normals': True,\n",
    "                'knn': 30,\n",
    "                **result\n",
    "            })\n",
    "    \n",
    "    # 2. Experiments with RANSAC initialization\n",
    "    print(\"\\nRunning experiments with RANSAC initialization...\")\n",
    "    voxel_size = 0.05  # For RANSAC\n",
    "    \n",
    "    # Prepare point clouds for RANSAC\n",
    "    source_down, target_down = prepare_point_clouds(source, target, voxel_size)\n",
    "    \n",
    "    # Prepare point clouds with normals for RANSAC\n",
    "    source_with_normals, target_with_normals = prepare_point_clouds(\n",
    "        source, target, voxel_size, estimate_normals=True)\n",
    "    \n",
    "    # RANSAC without normals\n",
    "    try:\n",
    "        ransac_initial = get_initial_transform_from_ransac(\n",
    "            source_down, target_down, voxel_size, with_normals=False)\n",
    "        \n",
    "        # Run experiments with RANSAC initialization (without normals)\n",
    "        for threshold in thresholds:\n",
    "            for max_iter in max_iterations:\n",
    "                print(f\"RANSAC Init, Threshold: {threshold}, Max Iterations: {max_iter}\")\n",
    "                result = point_to_point_icp(source, target, ransac_initial, \n",
    "                                           max_iter, threshold, estimate_normals=False)\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'initialization': 'RANSAC',\n",
    "                    'threshold': threshold,\n",
    "                    'max_iterations': max_iter,\n",
    "                    'with_normals': False,\n",
    "                    **result\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"RANSAC without normals failed: {e}\")\n",
    "    \n",
    "    # RANSAC with normals\n",
    "    try:\n",
    "        ransac_initial_with_normals = get_initial_transform_from_ransac(\n",
    "            source_with_normals, target_with_normals, voxel_size, with_normals=True)\n",
    "        \n",
    "        # Run experiments with RANSAC initialization (with normals)\n",
    "        for threshold in thresholds:\n",
    "            for max_iter in max_iterations:\n",
    "                print(f\"RANSAC Init (with normals), Threshold: {threshold}, Max Iterations: {max_iter}\")\n",
    "                result = point_to_point_icp(source, target, ransac_initial_with_normals, \n",
    "                                           max_iter, threshold, estimate_normals=True)\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'initialization': 'RANSAC with Normals',\n",
    "                    'threshold': threshold,\n",
    "                    'max_iterations': max_iter,\n",
    "                    'with_normals': True,\n",
    "                    'knn': 30,\n",
    "                    **result\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"RANSAC with normals failed: {e}\")\n",
    "    \n",
    "    # Convert results to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def analyze_results(results_df):\n",
    "    \"\"\"Analyze experimental results and find best hyperparameters\"\"\"\n",
    "    # Sort by final RMSE (ascending)\n",
    "    best_rmse = results_df.sort_values('final_rmse').iloc[0]\n",
    "    \n",
    "    # Sort by final fitness (descending)\n",
    "    best_fitness = results_df.sort_values('final_fitness', ascending=False).iloc[0]\n",
    "    \n",
    "    # Sort by RMSE improvement (descending)\n",
    "    best_rmse_improvement = results_df.sort_values('rmse_improvement', ascending=False).iloc[0]\n",
    "    \n",
    "    # Sort by fitness improvement (descending)\n",
    "    best_fitness_improvement = results_df.sort_values('fitness_improvement', ascending=False).iloc[0]\n",
    "    \n",
    "    # Group by initialization method\n",
    "    init_method_comparison = results_df.groupby('initialization').agg({\n",
    "        'initial_rmse': 'mean',\n",
    "        'final_rmse': 'mean',\n",
    "        'initial_fitness': 'mean',\n",
    "        'final_fitness': 'mean',\n",
    "        'rmse_improvement': 'mean',\n",
    "        'fitness_improvement': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Group by threshold\n",
    "    threshold_comparison = results_df.groupby('threshold').agg({\n",
    "        'initial_rmse': 'mean',\n",
    "        'final_rmse': 'mean',\n",
    "        'initial_fitness': 'mean',\n",
    "        'final_fitness': 'mean',\n",
    "        'rmse_improvement': 'mean',\n",
    "        'fitness_improvement': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Group by max_iterations\n",
    "    iterations_comparison = results_df.groupby('max_iterations').agg({\n",
    "        'initial_rmse': 'mean',\n",
    "        'final_rmse': 'mean',\n",
    "        'initial_fitness': 'mean',\n",
    "        'final_fitness': 'mean',\n",
    "        'rmse_improvement': 'mean',\n",
    "        'fitness_improvement': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Group by normals usage\n",
    "    normals_comparison = results_df.groupby('with_normals').agg({\n",
    "        'initial_rmse': 'mean',\n",
    "        'final_rmse': 'mean',\n",
    "        'initial_fitness': 'mean',\n",
    "        'final_fitness': 'mean',\n",
    "        'rmse_improvement': 'mean',\n",
    "        'fitness_improvement': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- EXPERIMENTAL RESULTS SUMMARY ---\")\n",
    "    \n",
    "    print(\"\\nBest configuration by final RMSE:\")\n",
    "    print(best_rmse[['initialization', 'threshold', 'max_iterations', 'with_normals', \n",
    "                    'initial_rmse', 'final_rmse', 'rmse_improvement']].to_string())\n",
    "    print(\"\\nTransformation matrix for best RMSE configuration:\")\n",
    "    print(best_rmse['transformation'])\n",
    "    \n",
    "    print(\"\\nBest configuration by final fitness:\")\n",
    "    print(best_fitness[['initialization', 'threshold', 'max_iterations', 'with_normals', \n",
    "                       'initial_fitness', 'final_fitness', 'fitness_improvement']].to_string())\n",
    "    \n",
    "    print(\"\\nComparison by initialization method:\")\n",
    "    print(tabulate(init_method_comparison, headers='keys', tablefmt='pretty'))\n",
    "    \n",
    "    print(\"\\nComparison by threshold value:\")\n",
    "    print(tabulate(threshold_comparison, headers='keys', tablefmt='pretty'))\n",
    "    \n",
    "    print(\"\\nComparison by max iterations:\")\n",
    "    print(tabulate(iterations_comparison, headers='keys', tablefmt='pretty'))\n",
    "    \n",
    "    print(\"\\nComparison by normal estimation usage:\")\n",
    "    print(tabulate(normals_comparison, headers='keys', tablefmt='pretty'))\n",
    "    \n",
    "    # Create summary table for report\n",
    "    summary_table = results_df[['initialization', 'threshold', 'max_iterations', 'with_normals',\n",
    "                               'initial_rmse', 'final_rmse', 'rmse_improvement',\n",
    "                               'initial_fitness', 'final_fitness', 'fitness_improvement']]\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "def visualize_best_result(source, target, best_transform):\n",
    "    \"\"\"Visualize point clouds with the best transformation\"\"\"\n",
    "    # Transform source to target using best transformation\n",
    "    source_transformed = copy.deepcopy(source)\n",
    "    source_transformed.transform(best_transform)\n",
    "    \n",
    "    # Visualize\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    source_temp.paint_uniform_color([1, 0, 0])  # Red for source\n",
    "    \n",
    "    target_temp = copy.deepcopy(target)\n",
    "    target_temp.paint_uniform_color([0, 1, 0])  # Green for target\n",
    "    \n",
    "    source_transformed_temp = copy.deepcopy(source_transformed)\n",
    "    source_transformed_temp.paint_uniform_color([0, 0, 1])  # Blue for transformed source\n",
    "    \n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp, source_transformed_temp])\n",
    "\n",
    "def main():\n",
    "    # Set the dataset directory path\n",
    "    dataset_dir = \"C:/Users/aarya/Github_Projects/Striver_A2Z/cv_bonus/selected_pcds\"  # Change this to your dataset path\n",
    "    \n",
    "    # Get sorted list of point cloud files\n",
    "    pcd_files = sorted([f for f in os.listdir(dataset_dir) if f.startswith(\"pointcloud_\") and f.endswith(\".pcd\")])\n",
    "    \n",
    "    # Select two consecutive point clouds for experiments\n",
    "    # (using clouds 10 and 11 as an example)\n",
    "    source_file = os.path.join(dataset_dir, pcd_files[10])\n",
    "    target_file = os.path.join(dataset_dir, pcd_files[11])\n",
    "    \n",
    "    print(f\"Running experiments on {pcd_files[10]} and {pcd_files[11]}\")\n",
    "    \n",
    "    # Run experiments\n",
    "    results_df = run_experiments(source_file, target_file)\n",
    "    \n",
    "    # Analyze results\n",
    "    summary_table = analyze_results(results_df)\n",
    "    \n",
    "    # Save summary table to CSV for reporting\n",
    "    summary_table.to_csv(\"icp_experiments_summary.csv\", index=False)\n",
    "    \n",
    "    # Visualize best result\n",
    "    best_transform = results_df.loc[results_df['final_rmse'].idxmin(), 'transformation']\n",
    "    source = load_point_cloud(source_file)\n",
    "    target = load_point_cloud(target_file)\n",
    "    visualize_best_result(source, target, best_transform)\n",
    "    \n",
    "    print(\"\\nExperiments completed! Results saved to 'icp_experiments_summary.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import copy\n",
    "import os\n",
    "\n",
    "def load_point_cloud(file_path):\n",
    "    \"\"\"Load a point cloud from file\"\"\"\n",
    "    pcd = o3d.io.read_point_cloud(file_path)\n",
    "    return pcd\n",
    "\n",
    "def visualize_registration_result(source, target, transformation):\n",
    "    \"\"\"\n",
    "    Visualize the source, target, and transformed source point clouds\n",
    "    \"\"\"\n",
    "    # Create a copy of the source point cloud and transform it\n",
    "    source_transformed = copy.deepcopy(source)\n",
    "    source_transformed.transform(transformation)\n",
    "    \n",
    "    # Color the point clouds\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    source_temp.paint_uniform_color([1, 0, 0])  # Red for original source\n",
    "    \n",
    "    target_temp = copy.deepcopy(target)\n",
    "    target_temp.paint_uniform_color([0, 1, 0])  # Green for target\n",
    "    \n",
    "    source_transformed_temp = copy.deepcopy(source_transformed)\n",
    "    source_transformed_temp.paint_uniform_color([0, 0, 1])  # Blue for transformed source\n",
    "    \n",
    "    # Create a coordinate frame to visualize the transformation\n",
    "    coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "        size=0.5, origin=[0, 0, 0])\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"Red: Original Source Point Cloud\")\n",
    "    print(\"Green: Target Point Cloud\")\n",
    "    print(\"Blue: Transformed Source Point Cloud\")\n",
    "    print(\"Coordinate frame shows the global reference frame\")\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp, source_transformed_temp, coord_frame])\n",
    "    \n",
    "    # Also visualize just the target and transformed source to see alignment quality\n",
    "    print(\"\\nVisualization of just the target (green) and transformed source (blue):\")\n",
    "    o3d.visualization.draw_geometries([target_temp, source_transformed_temp, coord_frame])\n",
    "    \n",
    "    # Calculate metrics to quantify the quality of alignment\n",
    "    evaluation = o3d.pipelines.registration.evaluate_registration(\n",
    "        source_transformed, target, 0.01)  # Using the best threshold\n",
    "    \n",
    "    print(\"\\nAlignment Metrics:\")\n",
    "    print(f\"Fitness: {evaluation.fitness:.6f}\")\n",
    "    print(f\"Inlier RMSE: {evaluation.inlier_rmse:.6f}\")\n",
    "    \n",
    "    return source_transformed\n",
    "\n",
    "def analyze_transformation(transformation):\n",
    "    \"\"\"\n",
    "    Analyze the transformation matrix and extract key components\n",
    "    \"\"\"\n",
    "    # Extract rotation matrix\n",
    "    rotation = transformation[:3, :3]\n",
    "    \n",
    "    # Extract translation vector\n",
    "    translation = transformation[:3, 3]\n",
    "    \n",
    "    # Check if rotation matrix is orthogonal\n",
    "    is_orthogonal = np.allclose(np.dot(rotation, rotation.T), np.eye(3), rtol=1e-5)\n",
    "    \n",
    "    # Calculate determinant to check if it's a proper rotation (det = 1)\n",
    "    determinant = np.linalg.det(rotation)\n",
    "    is_proper_rotation = np.isclose(determinant, 1.0)\n",
    "    \n",
    "    # Calculate the Euler angles from the rotation matrix (in degrees)\n",
    "    # These are ZYX Euler angles (yaw, pitch, roll)\n",
    "    if is_orthogonal:\n",
    "        yaw = np.arctan2(rotation[1, 0], rotation[0, 0])\n",
    "        pitch = np.arctan2(-rotation[2, 0], np.sqrt(rotation[2, 1]**2 + rotation[2, 2]**2))\n",
    "        roll = np.arctan2(rotation[2, 1], rotation[2, 2])\n",
    "        \n",
    "        # Convert to degrees\n",
    "        yaw_deg = np.degrees(yaw)\n",
    "        pitch_deg = np.degrees(pitch)\n",
    "        roll_deg = np.degrees(roll)\n",
    "    else:\n",
    "        yaw_deg, pitch_deg, roll_deg = None, None, None\n",
    "    \n",
    "    # Calculate the magnitude of translation\n",
    "    translation_magnitude = np.linalg.norm(translation)\n",
    "    \n",
    "    # Print the analysis\n",
    "    print(\"\\nTransformation Matrix Analysis:\")\n",
    "    print(f\"Translation vector: {translation}\")\n",
    "    print(f\"Translation magnitude: {translation_magnitude:.4f} units\")\n",
    "    \n",
    "    if is_orthogonal:\n",
    "        print(f\"Rotation is orthogonal: Yes\")\n",
    "        print(f\"Proper rotation (det = 1): {is_proper_rotation}\")\n",
    "        print(f\"Approximate rotation angles (degrees) - Roll: {roll_deg:.2f}, Pitch: {pitch_deg:.2f}, Yaw: {yaw_deg:.2f}\")\n",
    "    else:\n",
    "        print(\"Warning: Rotation matrix is not orthogonal\")\n",
    "    \n",
    "    return {\n",
    "        'translation': translation,\n",
    "        'translation_magnitude': translation_magnitude,\n",
    "        'is_orthogonal': is_orthogonal,\n",
    "        'is_proper_rotation': is_proper_rotation,\n",
    "        'angles_deg': (roll_deg, pitch_deg, yaw_deg) if is_orthogonal else None\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Set the dataset directory path\n",
    "    dataset_dir = \"C:/Users/aarya/Github_Projects/Striver_A2Z/cv_bonus/selected_pcds\"  # Change this to your dataset path\n",
    "    \n",
    "    # Get sorted list of point cloud files\n",
    "    pcd_files = sorted([f for f in os.listdir(dataset_dir) if f.startswith(\"pointcloud_\") and f.endswith(\".pcd\")])\n",
    "    \n",
    "    # Select two consecutive point clouds that match your experiment\n",
    "    # (Update indices as needed based on your experiments)\n",
    "    source_idx = 10\n",
    "    target_idx = 11\n",
    "    \n",
    "    source_file = os.path.join(dataset_dir, pcd_files[source_idx])\n",
    "    target_file = os.path.join(dataset_dir, pcd_files[target_idx])\n",
    "    \n",
    "    print(f\"Source point cloud: {pcd_files[source_idx]}\")\n",
    "    print(f\"Target point cloud: {pcd_files[target_idx]}\")\n",
    "    \n",
    "    # Load point clouds\n",
    "    source = load_point_cloud(source_file)\n",
    "    target = load_point_cloud(target_file)\n",
    "    \n",
    "    # Print basic information about the point clouds\n",
    "    print(f\"Source point cloud has {len(source.points)} points\")\n",
    "    print(f\"Target point cloud has {len(target.points)} points\")\n",
    "    \n",
    "    # Define the best transformation matrix from your experiments\n",
    "    # This is the \"best configuration by final RMSE\" from your results\n",
    "    best_transform = np.array([\n",
    "        [-0.46210878, -0.87531359, -0.14241349, 0.01718528],\n",
    "        [0.71034064, -0.46148236, 0.53146044, 0.47404969],\n",
    "        [0.53091586, -0.14443044, -0.83502586, 0.08875592],\n",
    "        [0.0, 0.0, 0.0, 1.0]\n",
    "    ])\n",
    "    \n",
    "    # Visualize the registration result\n",
    "    transformed_source = visualize_registration_result(source, target, best_transform)\n",
    "    \n",
    "    # Analyze the transformation\n",
    "    analysis = analyze_transformation(best_transform)\n",
    "    \n",
    "    # Save the transformed point cloud if needed\n",
    "    o3d.io.write_point_cloud(\"transformed_source.pcd\", transformed_source)\n",
    "    print(\"\\nTransformed point cloud saved as 'transformed_source.pcd'\")\n",
    "    \n",
    "    print(\"\\nConclusion:\")\n",
    "    print(\"\"\"\n",
    "    Reasons for the results:\n",
    "    1. The quality of registration depends heavily on the overlap between point clouds\n",
    "    2. Using normals estimation improved the alignment by providing additional geometric constraints\n",
    "    3. Lower threshold value (0.01) allowed for more precise matching of points\n",
    "    4. Higher max iterations (100) gave the algorithm more time to converge to the optimal solution\n",
    "    5. The transformation represents the TurtleBot's movement between the two frames\n",
    "       - Translation component shows how far the robot moved\n",
    "       - Rotation component shows how the robot changed orientation\n",
    "    6. Perfect or near-perfect alignment (RMSE â‰ˆ 0) suggests either:\n",
    "       - Extremely good registration performance, or\n",
    "       - Possible overfitting to noise if the point clouds were already very similar\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import copy\n",
    "import os\n",
    "from scipy.stats import ortho_group\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Function to load point clouds\n",
    "def load_point_cloud(file_path):\n",
    "    \"\"\"Load a point cloud from file and preprocess if needed\"\"\"\n",
    "    pcd = o3d.io.read_point_cloud(file_path)\n",
    "    return pcd\n",
    "\n",
    "def preprocess_point_cloud(pcd, voxel_size=0.05, estimate_normals=False, knn=30):\n",
    "    \"\"\"Preprocess point cloud with optional downsampling and normal estimation\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    processed_pcd = copy.deepcopy(pcd)\n",
    "    \n",
    "    # Downsample using voxel grid filter\n",
    "    processed_pcd = processed_pcd.voxel_down_sample(voxel_size)\n",
    "    \n",
    "    # Estimate normals if requested\n",
    "    if estimate_normals:\n",
    "        search_param = o3d.geometry.KDTreeSearchParamKNN(knn=knn)\n",
    "        processed_pcd.estimate_normals(search_param)\n",
    "        # Orient normals consistently\n",
    "        processed_pcd.orient_normals_consistent_tangent_plane(k=knn)\n",
    "    \n",
    "    return processed_pcd\n",
    "\n",
    "def point_to_point_icp(source, target, initial_transform=None, \n",
    "                       max_iteration=30, threshold=0.02, \n",
    "                       estimate_normals=False, knn=30):\n",
    "    \"\"\"Run point-to-point ICP with the provided parameters\"\"\"\n",
    "    # Process point clouds if needed\n",
    "    if estimate_normals:\n",
    "        source_processed = preprocess_point_cloud(source, voxel_size=0.05, estimate_normals=True, knn=knn)\n",
    "        target_processed = preprocess_point_cloud(target, voxel_size=0.05, estimate_normals=True, knn=knn)\n",
    "    else:\n",
    "        source_processed, target_processed = source, target\n",
    "    \n",
    "    # Default initial transform if not provided\n",
    "    if initial_transform is None:\n",
    "        # Using scipy's ortho_group to generate a random orthonormal matrix for rotation\n",
    "        R = ortho_group.rvs(3)\n",
    "        t = np.random.rand(3, 1) * 0.1  # Small random translation\n",
    "        \n",
    "        initial_transform = np.eye(4)\n",
    "        initial_transform[:3, :3] = R\n",
    "        initial_transform[:3, 3:] = t\n",
    "    \n",
    "    # Evaluate initial alignment\n",
    "    evaluation = o3d.pipelines.registration.evaluate_registration(\n",
    "        source_processed, target_processed, threshold, initial_transform)\n",
    "    initial_fitness = evaluation.fitness\n",
    "    initial_rmse = evaluation.inlier_rmse\n",
    "    \n",
    "    # Run ICP\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source_processed, target_processed, threshold, initial_transform,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=max_iteration))\n",
    "    \n",
    "    # Get results\n",
    "    transformation = reg_p2p.transformation\n",
    "    fitness = reg_p2p.fitness\n",
    "    inlier_rmse = reg_p2p.inlier_rmse\n",
    "    \n",
    "    return transformation, initial_fitness, initial_rmse, fitness, inlier_rmse\n",
    "\n",
    "# def register_all_point_clouds(dataset_dir, pcd_files, threshold=0.01, max_iteration=100, estimate_normals=True, knn=30):\n",
    "#     \"\"\"\n",
    "#     Register all point clouds in the sequence and compute the trajectory\n",
    "    \n",
    "#     Args:\n",
    "#         dataset_dir: Directory containing point cloud files\n",
    "#         pcd_files: List of point cloud filenames\n",
    "#         threshold: Distance threshold for ICP\n",
    "#         max_iteration: Maximum number of ICP iterations\n",
    "#         estimate_normals: Whether to estimate normals for registration\n",
    "#         knn: Number of nearest neighbors for normal estimation\n",
    "        \n",
    "#     Returns:\n",
    "#         registered_pcds: List of registered point clouds\n",
    "#         trajectory: List of transformation matrices representing the trajectory\n",
    "#         registration_results: DataFrame with registration metrics\n",
    "#     \"\"\"\n",
    "#     # Lists to store results\n",
    "#     trajectory = []  # To store poses (transformation matrices)\n",
    "#     registered_pcds = []  # To store registered point clouds\n",
    "#     registration_metrics = []  # To store registration metrics\n",
    "    \n",
    "#     # Load the first point cloud\n",
    "#     first_pcd = load_point_cloud(os.path.join(dataset_dir, pcd_files[0]))\n",
    "    \n",
    "#     # Add the first point cloud to the registered list\n",
    "#     registered_pcds.append(copy.deepcopy(first_pcd))\n",
    "    \n",
    "#     # Add the identity matrix as the first pose (global frame = first point cloud)\n",
    "#     global_transform = np.eye(4)\n",
    "#     trajectory.append(global_transform)\n",
    "    \n",
    "#     # Process each consecutive pair of point clouds\n",
    "#     for i in range(1, len(pcd_files)):\n",
    "#         print(f\"\\nProcessing point clouds {i-1} and {i}...\")\n",
    "        \n",
    "#         # Load the current point cloud\n",
    "#         current_pcd = load_point_cloud(os.path.join(dataset_dir, pcd_files[i]))\n",
    "        \n",
    "#         # Previous registered point cloud\n",
    "#         prev_pcd = load_point_cloud(os.path.join(dataset_dir, pcd_files[i-1]))\n",
    "        \n",
    "#         # Run ICP to find the transformation from current to previous\n",
    "#         transformation, initial_fitness, initial_rmse, final_fitness, final_rmse = point_to_point_icp(\n",
    "#             current_pcd, prev_pcd, \n",
    "#             initial_transform=None,  # Use random initialization\n",
    "#             max_iteration=max_iteration, \n",
    "#             threshold=threshold,\n",
    "#             estimate_normals=estimate_normals,\n",
    "#             knn=knn\n",
    "#         )\n",
    "        \n",
    "#         # Inverse the transformation because we want prev -> current (not current -> prev)\n",
    "#         # This gives us the motion of the robot\n",
    "#         relative_transform = np.linalg.inv(transformation)\n",
    "        \n",
    "#         # Update the global transformation\n",
    "#         global_transform = global_transform @ relative_transform\n",
    "        \n",
    "#         # Store the global transformation in the trajectory\n",
    "#         trajectory.append(copy.deepcopy(global_transform))\n",
    "        \n",
    "#         # Transform the current point cloud to the global frame\n",
    "#         transformed_pcd = copy.deepcopy(current_pcd)\n",
    "#         transformed_pcd.transform(global_transform)\n",
    "        \n",
    "#         # Add the transformed point cloud to the list\n",
    "#         registered_pcds.append(transformed_pcd)\n",
    "        \n",
    "#         # Store registration metrics\n",
    "#         registration_metrics.append({\n",
    "#             'pair': f\"{i-1}-{i}\",\n",
    "#             'source': pcd_files[i],\n",
    "#             'target': pcd_files[i-1],\n",
    "#             'initial_fitness': initial_fitness,\n",
    "#             'initial_rmse': initial_rmse,\n",
    "#             'final_fitness': final_fitness,\n",
    "#             'final_rmse': final_rmse,\n",
    "#             'improvement_fitness': final_fitness - initial_fitness,\n",
    "#             'improvement_rmse': initial_rmse - final_rmse\n",
    "#         })\n",
    "        \n",
    "#         print(f\"Registration {i}/{len(pcd_files)-1} - Fitness: {final_fitness:.6f}, RMSE: {final_rmse:.6f}\")\n",
    "    \n",
    "#     # Convert metrics to DataFrame\n",
    "#     metrics_df = pd.DataFrame(registration_metrics)\n",
    "    \n",
    "#     return registered_pcds, trajectory, metrics_df\n",
    "\n",
    "# (Keep visualization functions as before)\n",
    "\n",
    "# MODIFIED Sequence Registration Function\n",
    "def register_all_point_clouds(dataset_dir, pcd_files, \n",
    "                                          ransac_voxel_size=0.05, # Voxel size for RANSAC features/matching\n",
    "                                          icp_threshold=0.05,     # ICP correspondence threshold (maybe slightly larger)\n",
    "                                          icp_max_iteration=200, # More iterations for ICP\n",
    "                                          knn=30):\n",
    "    \"\"\"\n",
    "    Register all point clouds using RANSAC initial alignment + Point-to-Plane ICP refinement.\n",
    "    \"\"\"\n",
    "    trajectory = []\n",
    "    registered_pcds = []\n",
    "    registration_metrics = []\n",
    "\n",
    "    # Load and prepare the first point cloud\n",
    "    first_pcd_path = os.path.join(dataset_dir, pcd_files[0])\n",
    "    first_pcd = load_point_cloud(first_pcd_path)\n",
    "    # Preprocess first cloud (downsample, estimate normals) - needed as target for RANSAC/ICP\n",
    "    first_pcd_processed = preprocess_point_cloud(first_pcd, \n",
    "                                                 voxel_size=ransac_voxel_size, \n",
    "                                                 estimate_normals=True, knn=knn)\n",
    "    \n",
    "    registered_pcds.append(copy.deepcopy(first_pcd)) # Store original high-res cloud\n",
    "    global_transform = np.eye(4)\n",
    "    trajectory.append(global_transform)\n",
    "\n",
    "    prev_pcd_processed = first_pcd_processed # Keep track of the *processed* previous cloud\n",
    "\n",
    "    for i in range(1, len(pcd_files)):\n",
    "        print(f\"\\nProcessing point clouds {i-1} ({pcd_files[i-1]}) and {i} ({pcd_files[i]})...\")\n",
    "        \n",
    "        # Load current point cloud (original resolution)\n",
    "        current_pcd_path = os.path.join(dataset_dir, pcd_files[i])\n",
    "        current_pcd = load_point_cloud(current_pcd_path)\n",
    "\n",
    "        # Preprocess current point cloud for RANSAC and ICP\n",
    "        print(f\"Preprocessing cloud {i}...\")\n",
    "        current_pcd_processed = preprocess_point_cloud(current_pcd, \n",
    "                                                      voxel_size=ransac_voxel_size, \n",
    "                                                      estimate_normals=True, knn=knn)\n",
    "        \n",
    "        # --- RANSAC for Initial Alignment ---\n",
    "        print(\"Running RANSAC for initial alignment...\")\n",
    "        try:\n",
    "            # Use RANSAC with normals (FPFH features)\n",
    "            initial_transform_ransac = get_initial_transform_from_ransac(\n",
    "                current_pcd_processed, prev_pcd_processed, # Use processed clouds\n",
    "                ransac_voxel_size, \n",
    "                with_normals=True)\n",
    "            \n",
    "            # Evaluate RANSAC result (optional)\n",
    "            eval_ransac = o3d.pipelines.registration.evaluate_registration(\n",
    "                current_pcd_processed, prev_pcd_processed, ransac_voxel_size * 1.5, initial_transform_ransac)\n",
    "            print(f\"RANSAC Initial Guess - Fitness: {eval_ransac.fitness:.4f}, RMSE: {eval_ransac.inlier_rmse:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"RANSAC failed for pair {i-1}-{i}: {e}. Falling back to identity.\")\n",
    "            initial_transform_ransac = np.eye(4) # Fallback if RANSAC fails\n",
    "        # ------------------------------------\n",
    "\n",
    "        # --- Point-to-Plane ICP Refinement ---\n",
    "        print(\"Running Point-to-Plane ICP for refinement...\")\n",
    "        # Use the *processed* clouds for ICP, with RANSAC result as initial guess\n",
    "        transformation_icp, init_f, init_e, final_fitness, final_rmse = run_icp(\n",
    "            current_pcd_processed, prev_pcd_processed, \n",
    "            initial_transform=initial_transform_ransac,  # Use RANSAC result!\n",
    "            max_iteration=icp_max_iteration, \n",
    "            threshold=icp_threshold,\n",
    "            estimate_normals=True, # Already done, but ensures consistency\n",
    "            knn=knn,\n",
    "            voxel_size=ransac_voxel_size # Pass voxel size\n",
    "        )\n",
    "        # ------------------------------------\n",
    "        \n",
    "        # We found T aligning current_processed -> prev_processed\n",
    "        # Relative motion is inv(T)\n",
    "        if final_fitness < 0.1 and final_rmse > 0.1: # Basic check for bad registration\n",
    "             print(f\"Warning: Low fitness ({final_fitness:.3f}) and high RMSE ({final_rmse:.3f}) for pair {i-1}-{i}. Using Identity.\")\n",
    "             relative_transform = np.eye(4) # Use identity if ICP result is poor\n",
    "        else:\n",
    "             relative_transform = np.linalg.inv(transformation_icp)\n",
    "        \n",
    "        # Update global transform and trajectory\n",
    "        global_transform = global_transform @ relative_transform\n",
    "        trajectory.append(copy.deepcopy(global_transform))\n",
    "        \n",
    "        # Transform the *original* high-resolution current point cloud to global frame\n",
    "        transformed_pcd = copy.deepcopy(current_pcd)\n",
    "        transformed_pcd.transform(global_transform)\n",
    "        registered_pcds.append(transformed_pcd)\n",
    "        \n",
    "        # Store metrics\n",
    "        registration_metrics.append({\n",
    "            'pair': f\"{i-1}-{i}\",\n",
    "            'source': pcd_files[i],\n",
    "            'target': pcd_files[i-1],\n",
    "            # Note: init_f/init_e are from ICP start (after RANSAC), not absolute start\n",
    "            'initial_fitness_icp': init_f,\n",
    "            'initial_rmse_icp': init_e,\n",
    "            'final_fitness': final_fitness,\n",
    "            'final_rmse': final_rmse,\n",
    "            'improvement_fitness': final_fitness - init_f,\n",
    "            'improvement_rmse': init_e - final_rmse\n",
    "        })\n",
    "        \n",
    "        print(f\"Registration {i}/{len(pcd_files)-1} - ICP Fitness: {final_fitness:.6f}, ICP RMSE: {final_rmse:.6f}\")\n",
    "\n",
    "        # Update the previous processed cloud for the next iteration\n",
    "        prev_pcd_processed = current_pcd_processed\n",
    "            \n",
    "    metrics_df = pd.DataFrame(registration_metrics)\n",
    "    return registered_pcds, trajectory, metrics_df\n",
    "\n",
    "def save_trajectory_to_csv(trajectory, output_path):\n",
    "    \"\"\"\n",
    "    Save the trajectory to a CSV file\n",
    "    \n",
    "    Args:\n",
    "        trajectory: List of transformation matrices\n",
    "        output_path: Path to save the CSV file\n",
    "    \"\"\"\n",
    "    # Extract positions (translations) from transformation matrices\n",
    "    positions = []\n",
    "    for transform in trajectory:\n",
    "        # Extract X, Y, Z coordinates\n",
    "        x, y, z = transform[:3, 3]\n",
    "        \n",
    "        # Extract rotation matrix\n",
    "        rotation = transform[:3, :3]\n",
    "        \n",
    "        # Convert rotation to Euler angles (roll, pitch, yaw) - ZYX convention\n",
    "        # This is a simplified version - for precise conversion, consider using scipy.spatial.transform\n",
    "        yaw = np.arctan2(rotation[1, 0], rotation[0, 0])\n",
    "        pitch = np.arctan2(-rotation[2, 0], np.sqrt(rotation[2, 1]**2 + rotation[2, 2]**2))\n",
    "        roll = np.arctan2(rotation[2, 1], rotation[2, 2])\n",
    "        \n",
    "        # Convert to degrees\n",
    "        roll_deg, pitch_deg, yaw_deg = np.degrees(roll), np.degrees(pitch), np.degrees(yaw)\n",
    "        \n",
    "        positions.append({\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'z': z,\n",
    "            'roll': roll_deg,\n",
    "            'pitch': pitch_deg,\n",
    "            'yaw': yaw_deg\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    trajectory_df = pd.DataFrame(positions)\n",
    "    trajectory_df.to_csv(output_path, index_label='frame')\n",
    "    \n",
    "    return trajectory_df\n",
    "\n",
    "def plot_trajectory_3d(trajectory_df):\n",
    "    \"\"\"\n",
    "    Plot the 3D trajectory\n",
    "    \n",
    "    Args:\n",
    "        trajectory_df: DataFrame with trajectory positions\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot trajectory\n",
    "    ax.plot(trajectory_df['x'], trajectory_df['y'], trajectory_df['z'], 'b-', linewidth=2, label='Robot Path')\n",
    "    ax.scatter(trajectory_df['x'], trajectory_df['y'], trajectory_df['z'], c='red', s=50, label='Waypoints')\n",
    "    \n",
    "    # Mark start and end\n",
    "    ax.scatter(trajectory_df['x'].iloc[0], trajectory_df['y'].iloc[0], trajectory_df['z'].iloc[0], \n",
    "               c='green', s=100, label='Start')\n",
    "    ax.scatter(trajectory_df['x'].iloc[-1], trajectory_df['y'].iloc[-1], trajectory_df['z'].iloc[-1], \n",
    "               c='purple', s=100, label='End')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X (m)')\n",
    "    ax.set_ylabel('Y (m)')\n",
    "    ax.set_zlabel('Z (m)')\n",
    "    ax.set_title('TurtleBot 3D Trajectory')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('turtlebot_trajectory_3d.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Return figure for further modifications if needed\n",
    "    return fig, ax\n",
    "\n",
    "def plot_trajectory_2d(trajectory_df):\n",
    "    \"\"\"\n",
    "    Plot the 2D trajectory projection (top-down view)\n",
    "    \n",
    "    Args:\n",
    "        trajectory_df: DataFrame with trajectory positions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot trajectory\n",
    "    plt.plot(trajectory_df['x'], trajectory_df['y'], 'b-', linewidth=2, label='Robot Path')\n",
    "    plt.scatter(trajectory_df['x'], trajectory_df['y'], c='red', s=50, label='Waypoints')\n",
    "    \n",
    "    # Mark start and end\n",
    "    plt.scatter(trajectory_df['x'].iloc[0], trajectory_df['y'].iloc[0], c='green', s=100, label='Start')\n",
    "    plt.scatter(trajectory_df['x'].iloc[-1], trajectory_df['y'].iloc[-1], c='purple', s=100, label='End')\n",
    "    \n",
    "    # Set labels, title, and grid\n",
    "    plt.xlabel('X (m)')\n",
    "    plt.ylabel('Y (m)')\n",
    "    plt.title('TurtleBot 2D Trajectory (Top-Down View)')\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')  # Equal aspect ratio\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('turtlebot_trajectory_2d.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "def visualize_combined_point_cloud(registered_pcds, trajectory=None, downsample_voxel_size=0.05):\n",
    "    \"\"\"\n",
    "    Visualize the combined registered point cloud with the trajectory\n",
    "    \n",
    "    Args:\n",
    "        registered_pcds: List of registered point clouds\n",
    "        trajectory: List of transformation matrices for trajectory visualization\n",
    "        downsample_voxel_size: Size for downsampling to reduce visualization load\n",
    "    \"\"\"\n",
    "    # Create a combined point cloud map with random colors for each original cloud\n",
    "    map_vis = []\n",
    "    \n",
    "    # Process and add each registered point cloud\n",
    "    for i, pcd in enumerate(registered_pcds):\n",
    "        # Downsample the point cloud for visualization\n",
    "        pcd_down = pcd.voxel_down_sample(voxel_size=downsample_voxel_size)\n",
    "        pcd_colored = copy.deepcopy(pcd_down)\n",
    "        \n",
    "        # Assign a random color to each point cloud, but keep colors distinct\n",
    "        # Use HSV color space for better visual distinction\n",
    "        hue = i / len(registered_pcds)  # Evenly distributed hue values\n",
    "        # Convert HSV to RGB (simplified conversion)\n",
    "        if hue < 1/6:\n",
    "            r, g, b = 1, 6*hue, 0\n",
    "        elif hue < 2/6:\n",
    "            r, g, b = 2-6*hue, 1, 0\n",
    "        elif hue < 3/6:\n",
    "            r, g, b = 0, 1, 6*hue-2\n",
    "        elif hue < 4/6:\n",
    "            r, g, b = 0, 4-6*hue, 1\n",
    "        elif hue < 5/6:\n",
    "            r, g, b = 6*hue-4, 0, 1\n",
    "        else:\n",
    "            r, g, b = 1, 0, 6-6*hue\n",
    "        \n",
    "        pcd_colored.paint_uniform_color([r, g, b])\n",
    "        map_vis.append(pcd_colored)\n",
    "    \n",
    "    # Add trajectory visualization if provided\n",
    "    if trajectory is not None:\n",
    "        # Create a LineSet to represent the trajectory\n",
    "        line_set = o3d.geometry.LineSet()\n",
    "        \n",
    "        # Extract the translation part of each transformation matrix\n",
    "        points = np.array([T[:3, 3] for T in trajectory])\n",
    "        \n",
    "        # Set the points\n",
    "        line_set.points = o3d.utility.Vector3dVector(points)\n",
    "        \n",
    "        # Create lines connecting consecutive points\n",
    "        lines = [[i, i + 1] for i in range(len(points) - 1)]\n",
    "        line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "        \n",
    "        # Set colors (red)\n",
    "        colors = [[1, 0, 0] for _ in range(len(lines))]\n",
    "        line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "        \n",
    "        map_vis.append(line_set)\n",
    "        \n",
    "        # Add coordinate frames at key positions along trajectory\n",
    "        step = max(1, len(trajectory) // 10)  # Show ~10 frames\n",
    "        for i in range(0, len(trajectory), step):\n",
    "            coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "                size=0.3, origin=[0, 0, 0])\n",
    "            coord_frame.transform(trajectory[i])\n",
    "            map_vis.append(coord_frame)\n",
    "    \n",
    "    # Visualize\n",
    "    o3d.visualization.draw_geometries(map_vis)\n",
    "    \n",
    "    # Create and save a combined point cloud\n",
    "    combined_pcd = o3d.geometry.PointCloud()\n",
    "    for pcd in registered_pcds:\n",
    "        combined_pcd += pcd.voxel_down_sample(voxel_size=downsample_voxel_size)\n",
    "    \n",
    "    # Save the combined point cloud\n",
    "    o3d.io.write_point_cloud(\"global_registered_pointcloud.pcd\", combined_pcd)\n",
    "    print(\"Saved global registered point cloud to 'global_registered_pointcloud.pcd'\")\n",
    "\n",
    "def main():\n",
    "    dataset_dir = \"C:/Users/aarya/Github_Projects/Striver_A2Z/cv_bonus/selected_pcds\"\n",
    "    pcd_files = sorted([f for f in os.listdir(dataset_dir) if f.startswith(\"pointcloud_\") and f.endswith(\".pcd\")])\n",
    "    print(f\"Found {len(pcd_files)} point cloud files.\")\n",
    "\n",
    "    # --- Adjusted Parameters ---\n",
    "    ransac_voxel_size = 0.05 # Controls RANSAC speed/granularity\n",
    "    icp_threshold = 0.05     # A bit more lenient than 0.01\n",
    "    icp_max_iteration = 200  # More iterations\n",
    "    knn_normals = 30         # For normal estimation\n",
    "    # -------------------------\n",
    "\n",
    "    print(\"\\nRegistering all point clouds using RANSAC + Point-to-Plane ICP:\")\n",
    "    print(f\"- RANSAC Voxel Size: {ransac_voxel_size}\")\n",
    "    print(f\"- ICP Distance threshold: {icp_threshold}\")\n",
    "    print(f\"- ICP Max iterations: {icp_max_iteration}\")\n",
    "    print(f\"- KNN for normals: {knn_normals}\")\n",
    "\n",
    "    # --- Call the NEW function ---\n",
    "    registered_pcds, trajectory, metrics_df = register_all_point_clouds_with_ransac(\n",
    "        dataset_dir, pcd_files, \n",
    "        ransac_voxel_size=ransac_voxel_size, \n",
    "        icp_threshold=icp_threshold, \n",
    "        icp_max_iteration=icp_max_iteration, \n",
    "        knn=knn_normals\n",
    "    )\n",
    "    # -----------------------------\n",
    "\n",
    "    print(\"\\nRegistration complete!\")\n",
    "    print(f\"Registered {len(registered_pcds)} point clouds.\")\n",
    "    print(f\"Trajectory has {len(trajectory)} poses.\")\n",
    "\n",
    "    # --- Rest of the analysis, plotting, saving remains the same ---\n",
    "    metrics_df.to_csv(\"registration_metrics_ransac_p2plane.csv\", index=False) # Save with new name\n",
    "    print(\"Saved registration metrics to 'registration_metrics_ransac_p2plane.csv'\")\n",
    "    \n",
    "    trajectory_df = save_trajectory_to_csv(trajectory, \"turtlebot_trajectory_ransac_p2plane.csv\") # Save with new name\n",
    "    print(\"Saved trajectory to 'turtlebot_trajectory_ransac_p2plane.csv'\")\n",
    "    \n",
    "    plot_trajectory_3d(trajectory_df)\n",
    "    plot_trajectory_2d(trajectory_df)\n",
    "    print(\"Created trajectory plots: 'turtlebot_trajectory_3d.png' and 'turtlebot_trajectory_2d.png' (overwritten)\")\n",
    "    \n",
    "    print(\"\\nVisualizing the registered point cloud map and trajectory...\")\n",
    "    visualize_combined_point_cloud(registered_pcds, trajectory, downsample_voxel_size=0.05) \n",
    "    # Saves to global_registered_pointcloud.pcd (overwritten)\n",
    "    \n",
    "    print(\"\\nSummary of Registration Results:\")\n",
    "    print(f\"Average final fitness: {metrics_df['final_fitness'].mean():.4f}\")\n",
    "    print(f\"Average final RMSE: {metrics_df['final_rmse'].mean():.4f}\")\n",
    "    # Calculate distance from the new trajectory_df\n",
    "    total_distance = np.sum(np.sqrt(np.diff(trajectory_df['x'])**2 + np.diff(trajectory_df['y'])**2 + np.diff(trajectory_df['z'])**2))\n",
    "    print(f\"Total trajectory distance: {total_distance:.4f} meters\")\n",
    "\n",
    "        # Save a summary report\n",
    "    with open(\"registration_summary.txt\", \"w\") as f:\n",
    "        f.write(\"Point Cloud Registration Summary\\n\")\n",
    "        f.write(\"==============================\\n\\n\")\n",
    "        f.write(f\"Number of point clouds processed: {len(pcd_files)}\\n\")\n",
    "        f.write(f\"Best hyperparameters used:\\n\")\n",
    "        f.write(f\"- Distance threshold: {threshold}\\n\")\n",
    "        f.write(f\"- Max iterations: {max_iteration}\\n\")\n",
    "        f.write(f\"- Using normal estimation: {estimate_normals}\\n\")\n",
    "        f.write(f\"- KNN for normals: {knn}\\n\\n\")\n",
    "        f.write(f\"Registration Results:\\n\")\n",
    "        f.write(f\"- Average fitness: {metrics_df['final_fitness'].mean():.4f}\\n\")\n",
    "        f.write(f\"- Average RMSE: {metrics_df['final_rmse'].mean():.4f}\\n\")\n",
    "        f.write(f\"- Average fitness improvement: {metrics_df['improvement_fitness'].mean():.4f}\\n\")\n",
    "        f.write(f\"- Average RMSE improvement: {metrics_df['improvement_rmse'].mean():.4f}\\n\\n\")\n",
    "        f.write(f\"Trajectory Information:\\n\")\n",
    "        f.write(f\"- Total distance traveled: {np.sum(np.sqrt(np.diff(trajectory_df['x'])**2 + np.diff(trajectory_df['y'])**2 + np.diff(trajectory_df['z'])**2)):.4f} meters\\n\")\n",
    "        f.write(f\"- Bounding box (X): {trajectory_df['x'].min():.2f} to {trajectory_df['x'].max():.2f} meters\\n\")\n",
    "        f.write(f\"- Bounding box (Y): {trajectory_df['y'].min():.2f} to {trajectory_df['y'].max():.2f} meters\\n\")\n",
    "        f.write(f\"- Bounding box (Z): {trajectory_df['z'].min():.2f} to {trajectory_df['z'].max():.2f} meters\\n\")\n",
    "    print(\"\\nSaved summary report to 'registration_summary.txt'\")\n",
    "    print(\"\\nAll tasks completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
